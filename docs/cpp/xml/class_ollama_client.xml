<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.15.0" xml:lang="en-US">
  <compounddef id="class_ollama_client" kind="class" language="C++" prot="public">
    <compoundname>OllamaClient</compoundname>
    <includes refid="_ollama_client_8h" local="no">OllamaClient.h</includes>
    <sectiondef kind="public-static-func">
      <memberdef kind="function" id="class_ollama_client_1ad910476e1f269b575dc16221dec246ff" prot="public" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>std::string</type>
        <definition>std::string OllamaClient::RunSimple</definition>
        <argsstring>(const std::string &amp;modelName, const std::string &amp;prompt, const OllamaSettings &amp;settings)</argsstring>
        <name>RunSimple</name>
        <qualifiedname>OllamaClient::RunSimple</qualifiedname>
        <param>
          <type>const std::string &amp;</type>
          <declname>modelName</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>prompt</declname>
        </param>
        <param>
          <type>const <ref refid="struct_ollama_settings" kindref="compound">OllamaSettings</ref> &amp;</type>
          <declname>settings</declname>
        </param>
        <briefdescription>
<para>Simple helper function to call Ollama HTTP API. </para>
        </briefdescription>
        <detaileddescription>
<para>Makes a single HTTP request to Ollama with the given model and prompt. Uses the provided <ref refid="struct_ollama_settings" kindref="compound">OllamaSettings</ref> for connection configuration.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>modelName</parametername>
</parameternamelist>
<parameterdescription>
<para>Name of the LLM model (e.g., &quot;llama3&quot;, &quot;mistral&quot;) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>prompt</parametername>
</parameternamelist>
<parameterdescription>
<para>Prompt text to send to the LLM </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>settings</parametername>
</parameternamelist>
<parameterdescription>
<para>Ollama connection settings (host, port, timeouts) </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>JSON response as a string, or empty string on error</para>
</simplesect>
<simplesect kind="note"><para>This function does not retry on failure </para>
</simplesect>
<simplesect kind="note"><para>Uses WinHTTP on Windows for HTTP communication </para>
</simplesect>
<simplesect kind="see"><para><ref refid="class_ollama_client_1a1fea956fd3e63b9df3cb2184731ef692" kindref="member">RunWithRetry</ref> for automatic retry logic </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="RundeeItemFactory/RundeeItemFactory/include/Clients/OllamaClient.h" line="42" column="24" bodyfile="RundeeItemFactory/RundeeItemFactory/src/Clients/OllamaClient.cpp" bodystart="183" bodyend="403"/>
        <references refid="struct_ollama_settings_1a48de5e10f2a8d2ecad02b9e5828190ba" compoundref="_app_config_8h" startline="57">OllamaSettings::connectTimeoutMs</references>
        <references refid="struct_ollama_settings_1ae133460b157e143a07be3bfc6aeecc95" compoundref="_app_config_8h" startline="29">OllamaSettings::host</references>
        <references refid="struct_ollama_settings_1a0a67dca1971d55fe16b3e7e8543c1df2" compoundref="_app_config_8h" startline="36">OllamaSettings::port</references>
        <references refid="struct_ollama_settings_1ad2fe237d5c8721c659f68026c4e419f3" compoundref="_app_config_8h" startline="71">OllamaSettings::receiveTimeoutMs</references>
        <references refid="struct_ollama_settings_1a86f734afb09e501f8b7e3f96338b6de8" compoundref="_app_config_8h" startline="50">OllamaSettings::requestTimeoutSeconds</references>
        <references refid="struct_ollama_settings_1abd30d9027b9477043fec7697c42c2e13" compoundref="_app_config_8h" startline="64">OllamaSettings::sendTimeoutMs</references>
        <referencedby refid="class_ollama_client_1a1fea956fd3e63b9df3cb2184731ef692" compoundref="_ollama_client_8cpp" startline="405" endline="479">RunWithRetry</referencedby>
      </memberdef>
      <memberdef kind="function" id="class_ollama_client_1a1fea956fd3e63b9df3cb2184731ef692" prot="public" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>std::string</type>
        <definition>std::string OllamaClient::RunWithRetry</definition>
        <argsstring>(const std::string &amp;modelName, const std::string &amp;prompt, int maxRetries=3, int timeoutSeconds=120)</argsstring>
        <name>RunWithRetry</name>
        <qualifiedname>OllamaClient::RunWithRetry</qualifiedname>
        <param>
          <type>const std::string &amp;</type>
          <declname>modelName</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>prompt</declname>
        </param>
        <param>
          <type>int</type>
          <declname>maxRetries</declname>
          <defval>3</defval>
        </param>
        <param>
          <type>int</type>
          <declname>timeoutSeconds</declname>
          <defval>120</defval>
        </param>
        <briefdescription>
<para>Run LLM call with automatic retry logic. </para>
        </briefdescription>
        <detaileddescription>
<para>Attempts to call the LLM with automatic retry on failure. Uses settings from <ref refid="class_app_config" kindref="compound">AppConfig</ref> (loaded from config/rundee_config.json).</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>modelName</parametername>
</parameternamelist>
<parameterdescription>
<para>Name of the LLM model (e.g., &quot;llama3&quot;, &quot;mistral&quot;) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>prompt</parametername>
</parameternamelist>
<parameterdescription>
<para>Prompt text to send to the LLM </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>maxRetries</parametername>
</parameternamelist>
<parameterdescription>
<para>Maximum number of retry attempts (default: 3) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>timeoutSeconds</parametername>
</parameternamelist>
<parameterdescription>
<para>Timeout per attempt in seconds (default: 120, 0 = no timeout) </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>Response string, or empty string if all attempts failed</para>
</simplesect>
<simplesect kind="note"><para>Automatically loads <ref refid="struct_ollama_settings" kindref="compound">OllamaSettings</ref> from <ref refid="class_app_config" kindref="compound">AppConfig</ref> </para>
</simplesect>
<simplesect kind="note"><para>Retries with exponential backoff on failure </para>
</simplesect>
<simplesect kind="note"><para>Logs retry attempts to std::cerr </para>
</simplesect>
<simplesect kind="see"><para><ref refid="class_ollama_client_1ad910476e1f269b575dc16221dec246ff" kindref="member">RunSimple</ref> for single-call version </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="RundeeItemFactory/RundeeItemFactory/include/Clients/OllamaClient.h" line="63" column="24" bodyfile="RundeeItemFactory/RundeeItemFactory/src/Clients/OllamaClient.cpp" bodystart="405" bodyend="479"/>
        <references refid="class_app_config_1a8405660553a49926e88f255a84b34c7b" compoundref="_app_config_8cpp" startline="151" endline="155">AppConfig::GetOllamaSettings</references>
        <references refid="struct_ollama_settings_1af7b0acac83343c2b079c9c612b23cc0a" compoundref="_app_config_8h" startline="43">OllamaSettings::maxRetries</references>
        <references refid="struct_ollama_settings_1a86f734afb09e501f8b7e3f96338b6de8" compoundref="_app_config_8h" startline="50">OllamaSettings::requestTimeoutSeconds</references>
        <references refid="class_ollama_client_1ad910476e1f269b575dc16221dec246ff" compoundref="_ollama_client_8cpp" startline="183" endline="403">RunSimple</references>
        <referencedby refid="namespace_item_generator_1abcd9165612d17e63efd704c0e85b2967" compoundref="_item_generator_8cpp" startline="1808" endline="1920">ItemGenerator::GenerateWithLLM_SingleBatch</referencedby>
        <referencedby refid="_item_generator_8cpp_1aef3c20f7b6d0f2803e617ab0f8fcdcec" compoundref="_item_generator_8cpp" startline="1095" endline="1508">ProcessLLMResponse_Ammo</referencedby>
        <referencedby refid="_item_generator_8cpp_1aaea5114ff8a34af218dcdf247cbc3fda" compoundref="_item_generator_8cpp" startline="681" endline="1093">ProcessLLMResponse_WeaponComponent</referencedby>
      </memberdef>
    </sectiondef>
    <briefdescription>
<para>Static class for communicating with Ollama LLM server. </para>
    </briefdescription>
    <detaileddescription>
<para>Provides methods to send prompts to Ollama and receive JSON responses. Uses HTTP API (WinHTTP on Windows) for communication. </para>
    </detaileddescription>
    <location file="RundeeItemFactory/RundeeItemFactory/include/Clients/OllamaClient.h" line="24" column="1" bodyfile="RundeeItemFactory/RundeeItemFactory/include/Clients/OllamaClient.h" bodystart="25" bodyend="67"/>
    <listofallmembers>
      <member refid="class_ollama_client_1ad910476e1f269b575dc16221dec246ff" prot="public" virt="non-virtual"><scope>OllamaClient</scope><name>RunSimple</name></member>
      <member refid="class_ollama_client_1a1fea956fd3e63b9df3cb2184731ef692" prot="public" virt="non-virtual"><scope>OllamaClient</scope><name>RunWithRetry</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
